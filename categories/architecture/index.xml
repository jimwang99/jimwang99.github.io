<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Architecture on When Moore&#39;s Law ENDS</title>
    <link>//localhost:1313/categories/architecture/</link>
    <description>Recent content in Architecture on When Moore&#39;s Law ENDS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="//localhost:1313/categories/architecture/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Huwcha Accelerator architecture</title>
      <link>//localhost:1313/blog/architecture/2019-01-03-hwacha-accelerator/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/blog/architecture/2019-01-03-hwacha-accelerator/</guid>
      <description>From the reading of this paper, &amp;ldquo;The Hwacha Microarchitecture Manual, Version 3.8.1&amp;rdquo;, I found out that our Pygmy ES1 architecture is almost the same idea, just not as fancy.
 We don&amp;rsquo;t have cache coherency, because we operate on unified physical memory space The vector execution unit is not as fancy, just useless multithreading, no systolic The prefetch is supposed to be done by DMA engine that is manually controlled by software  System architecture  The vector accelerator only has L1 I\$, no D\$  Don&amp;rsquo;t need to maintain cache coherency Lots of vector registers, 512 in total, each is 64x2x4=512-bit Wide bus connection to L2$, to provide higher bandwidth  Uncached TileLink between L1 I\$ (both scalar processor and vector processors) and L2\$ Cached TileLink between L1 D\$ (only in scalar processor)  L2\$ maintains directory bits, which determines the states of corresponding cache line (JW: maybe something like MESI bits)  Operations of L2\$, supported by TileLink protocol (&amp;ldquo;Productive Design of Extensible On-Chip Memory Hierarchies&amp;rdquo;)  Sub-cache-block accesses Data prefetch requests Read from DDR, don&amp;rsquo;t need to send back to the requester Atomic memory operations ALU inside L2 cache banks   Decoupling  Access/execute decoupling Decoupled vector arch Cache refill/access decoupling  Vector Command Queue (VCMDQ)  Instruction fetch is handled by scalar processor, and then sent to VCMDQ  There is explicity defined start vf and stop vstop instructions that flags the begin and end of vector instructions JW: why is that necessary?</description>
    </item>
    
    <item>
      <title>Note of RISC-V Vector ISA Spec v0.6</title>
      <link>//localhost:1313/blog/architecture/2018-12-15-riscv-v-spec/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/blog/architecture/2018-12-15-riscv-v-spec/</guid>
      <description>Vector regfile  32 of them, v0 to v31 Each is VLEN bits Each can be divided into several elements  The max element width is ELEN CSR vsew maps to SEW (standard element width) controls their width dynamically CSR vl controls the number of elements to operate on for vector instructions  Packing of shorter vector  when SEW is smaller than ELEN, multiple SEW will be packed into one ELEN unit Following little-endian rule ELEN units are packed into VLEN register also Following little-endiam rule  Storage of longer vector  If operand longer than SEW is needed, then Even-numbered vector register holds the even-numbered elements Odd-numbered vector register holds the odd-numberred elements WHY?</description>
    </item>
    
    <item>
      <title>Ariane (PULP series high-performance core)</title>
      <link>//localhost:1313/blog/architecture/2018-12-01-ariane-cpu/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/blog/architecture/2018-12-01-ariane-cpu/</guid>
      <description>Architecture note PC gen stage  The fetching address for i-cache is always word-aligned.  Fetch stage  Its fetch stage doesn&amp;rsquo;t have much decoding work to do, only the necessary one to generate next PC. And it relies on its branch prediction to give out next PC.
 There is an internal FIFO with 2 entries to log the PC (and other meta-info) that was sent to i-cache, while waiting for its response.</description>
    </item>
    
    <item>
      <title>CPU Architecture Notes</title>
      <link>//localhost:1313/blog/architecture/2018-11-01-cpu-architecture-notes/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/blog/architecture/2018-11-01-cpu-architecture-notes/</guid>
      <description>Register renaming To eliminate the false and output data dependency by adding extra physical registers more than architectural registers.
 Read-after-write (RAW) is true data dependency Write-after-write (WAW) is output data dependency Write-after-read (WAR) is false data dependency  Superscalar Dynamically issue multiple instructions in each cycle to increase IPC.
 Normally need multi-port register files and ALU to avoid structural hazard. Can be in-order or out-of-order  Re-order buffer For out-of-order execution CPU architecture, results are put into re-order buffer waiting for commit.</description>
    </item>
    
    <item>
      <title>Cache coherence</title>
      <link>//localhost:1313/blog/architecture/2018-11-01-cache-coherence/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/blog/architecture/2018-11-01-cache-coherence/</guid>
      <description>Coherence mechanism Snooping Every cache maintain its own cache state. And when it needs to access a shared address space, it sends snooping messages to all the other caches to either update or invalidate them.
 Write invalidate: write operation will invalidate all the other shared copies. Others will have to read again from the next level of cache to use it again. Write update: write operation will give the written data to the shared copies and update them accordingly.</description>
    </item>
    
  </channel>
</rss>